{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968f554-8622-4f2d-abdd-66ad2e10bbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc1660e-a0bc-41aa-9151-3d6030fd5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other modules not related to PySpark\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from datetime import *\n",
    "import statistics as stats\n",
    "# This helps auto print out the items without explixitly using 'print'\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad22f5e-8a91-4b58-93cb-5052275be51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark related modules\n",
    "import pyspark\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import lit, desc, col, size, array_contains\\\n",
    ", isnan, udf, hour, array_min, array_max, countDistinct\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "MAX_MEMORY = '15G'\n",
    "# Initialize a spark session.\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n",
    "        .set('spark.executor.heartbeatInterval', 10000) \\\n",
    "        .set('spark.network.timeout', 10000) \\\n",
    "        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n",
    "        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "        .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Pyspark guide\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()\n",
    "filename_data = 'data/endomondoHR.json'\n",
    "# Load the main data set into pyspark data frame \n",
    "df = spark.read.json(filename_data, mode=\"DROPMALFORMED\")\n",
    "print('Data frame type: ' + str(type(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c3ed5-4529-4a46-a1f2-4a4825c6b43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
